{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Assignment : 2\n",
    "### Mohammed Intekhab : 2018AD04076\n",
    "### Ajay Sharma       : 2018AD04064\n",
    "### Avinash Chandra   : 2018AD04061"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial and Computational Intelligence (Assignment - 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "As part of the 2nd Assignment, we'll try and predict the Part of Speech (POS) tag for each word in a provided sentence.\n",
    "\n",
    "You are required to build a model using Hidden Markov Models which would help you predict the POS tags for all words in an utterance.\n",
    "\n",
    "### What is a POS tag?\n",
    "\n",
    "In corpus linguistics, part-of-speech tagging (POS tagging or PoS tagging or POST), also called grammatical tagging or word-category disambiguation, is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its contextâ€”i.e., its relationship with adjacent and related words in a phrase, sentence, or paragraph. A simplified form of this is commonly taught to school-age children, in the identification of words as nouns, verbs, adjectives, adverbs, etc.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The dataset can be downloaded from https://drive.google.com/open?id=1345iaxqTImJN6mKGh_c1T5n2OWumpyYz. You can access it only using your BITS IDs.\n",
    "\n",
    "#### Dataset Description\n",
    "##### Sample Tuple\n",
    "b100-5507\n",
    "\n",
    "Mr.\tNOUN\n",
    "<br>\n",
    "Podger\tNOUN\n",
    "<br>\n",
    "had\tVERB\n",
    "<br>\n",
    "thanked\tVERB\n",
    "<br>\n",
    "him\tPRON\n",
    "<br>\n",
    "gravely\tADV\n",
    "<br>\n",
    ",\t.\n",
    "<br>\n",
    "and\tCONJ\n",
    "<br>\n",
    "now\tADV\n",
    "<br>\n",
    "he\tPRON\n",
    "<br>\n",
    "made\tVERB\n",
    "<br>\n",
    "use\tNOUN\n",
    "<br>\n",
    "of\tADP\n",
    "<br>\n",
    "the\tDET\n",
    "<br>\n",
    "advice\tNOUN\n",
    "<br>\n",
    ".\t.\n",
    "<br>\n",
    "##### Explanation\n",
    "The first token \"b100-5507\" is just a key and acts like an identifier to indicate the beginning of a sentence.\n",
    "<br>\n",
    "The other tokens have a (Word, POS Tag) pairing.\n",
    "\n",
    "__List of POS Tags are:__\n",
    ".\n",
    "<br>\n",
    "ADJ\n",
    "<br>\n",
    "ADP\n",
    "<br>\n",
    "ADV\n",
    "<br>\n",
    "CONJ\n",
    "<br>\n",
    "DET\n",
    "<br>\n",
    "NOUN\n",
    "<br>\n",
    "NUM\n",
    "<br>\n",
    "PRON\n",
    "<br>\n",
    "PRT\n",
    "<br>\n",
    "VERB\n",
    "<br>\n",
    "X\n",
    "\n",
    "__Note__\n",
    "<br>\n",
    "__.__ is used to indicate special characters such as '.', ','\n",
    "<br>\n",
    "__X__ is used to indicate vocab not part of Enlish Language mostly.\n",
    "Others are Standard POS tags.\n",
    "\n",
    "### Evaluation\n",
    "We wish to evaluate based on \n",
    "- coding practices being followed\n",
    "- commenting to explain the code and logic behind doing something\n",
    "- your understanding and explanation of data\n",
    "- how good the model would perform on unseen data.\n",
    "\n",
    "### Train-Test Split\n",
    "Let us use a 80-20 split of our data for training and evaluation purpose.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mplimg\n",
    "from IPython.core.display import HTML\n",
    "from collections import Counter, defaultdict, namedtuple, OrderedDict\n",
    "from pomegranate import State, HiddenMarkovModel, DiscreteDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to Read data\n",
    "Sentence = namedtuple(\"Sentence\", \"words tags\")\n",
    "\n",
    "def read_data(filename):\n",
    "    \"\"\"Read tagged sentence data\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        sentence_lines = [l.split(\"\\n\") for l in f.read().split(\"\\n\\n\")]\n",
    "    return OrderedDict(((s[0], Sentence(*zip(*[l.strip().split(\"\\t\")\n",
    "                        for l in s[1:]]))) for s in sentence_lines if s[0]))\n",
    "\n",
    "def read_tags(filename):\n",
    "    \"\"\"Read a list of word tag classes\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        tags = f.read().split(\"\\n\")\n",
    "    return frozenset(tags)\n",
    "\n",
    "class Subset(namedtuple(\"BaseSet\", \"sentences keys vocab X tagset Y N stream\")):\n",
    "    def __new__(cls, sentences, keys):\n",
    "        word_sequences = tuple([sentences[k].words for k in keys])\n",
    "        tag_sequences = tuple([sentences[k].tags for k in keys])\n",
    "        wordset = frozenset(chain(*word_sequences))\n",
    "        tagset = frozenset(chain(*tag_sequences))\n",
    "        N = sum(1 for _ in chain(*(sentences[k].words for k in keys)))\n",
    "        stream = tuple(zip(chain(*word_sequences), chain(*tag_sequences)))\n",
    "        return super().__new__(cls, {k: sentences[k] for k in keys}, keys, wordset, word_sequences,\n",
    "                               tagset, tag_sequences, N, stream.__iter__)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.sentences.items())\n",
    "\n",
    "\n",
    "class Dataset(namedtuple(\"_Dataset\", \"sentences keys vocab X tagset Y training_set testing_set N stream\")):\n",
    "    def __new__(cls, tagfile, datafile, train_test_split=0.8, seed=46675):\n",
    "        tagset = read_tags(tagfile)\n",
    "        sentences = read_data(datafile)\n",
    "        keys = tuple(sentences.keys())\n",
    "        wordset = frozenset(chain(*[s.words for s in sentences.values()]))\n",
    "        word_sequences = tuple([sentences[k].words for k in keys])\n",
    "        tag_sequences = tuple([sentences[k].tags for k in keys])\n",
    "        N = sum(1 for _ in chain(*(s.words for s in sentences.values())))\n",
    "        \n",
    "        # split data into train/test sets\n",
    "        _keys = list(keys)\n",
    "        if seed is not None: random.seed(seed)\n",
    "        random.shuffle(_keys)\n",
    "        split = int(train_test_split * len(_keys))\n",
    "        training_data = Subset(sentences, _keys[:split])\n",
    "        testing_data = Subset(sentences, _keys[split:])\n",
    "        stream = tuple(zip(chain(*word_sequences), chain(*tag_sequences)))\n",
    "        return super().__new__(cls, dict(sentences), keys, wordset, word_sequences, tagset,\n",
    "                               tag_sequences, training_data, testing_data, N, stream.__iter__)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.sentences.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Training Data = 80% , Test Data = 20%\n",
    "2. List of POS Tags are present in the file `all-tags.txt`.\n",
    "3. The input file has been downloaded and saved as `data.txt`.\n",
    "4. Basic stats about input file data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset(\"all-tags.txt\", \"data.txt\", train_test_split=0.8)\n",
    "print(\"There are {} sentences in the corpus.\".format(len(data)))\n",
    "print(\"There are {} sentences in the training set.\".format(len(data.training_set)))\n",
    "print(\"There are {} sentences in the testing set.\".format(len(data.testing_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are a total of {} samples of {} unique words in the corpus.\".format(data.N, len(data.vocab)))\n",
    "print(\"There are {} samples of {} unique words in the training set.\".format(data.training_set.N, len(data.training_set.vocab)))\n",
    "print(\"There are {} samples of {} unique words in the testing set.\".format(data.testing_set.N, len(data.testing_set.vocab)))\n",
    "print(\"There are {} words in the test set that are missing in the training set.\".format(len(data.testing_set.vocab - data.training_set.vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentences\n",
    "- the input file is read into a dictionary where the first identifier is dictionary key. `Example (b100-38532)`\n",
    "- for each key the value is a tuple of (words,tag) for that sentence. Example the value for key `b100-38532` is\n",
    "- (words=('Perhaps', 'it', 'was', 'right', ';', ';'), tags=('ADV', 'PRON', 'VERB', 'ADJ', '.', '.'))\n",
    "- `Dataset.sentences` is a dictionary of all sentences in the training set from `data.txt`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example \n",
    "key = 'b100-5507'\n",
    "print(f\"Sentence: {key}\")\n",
    "print(f\"words:\\n\\t{data.sentences[key].words}\")\n",
    "print(f\"tags:\\n\\t{data.sentences[key].tags}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions \n",
    "- calculate unigram , bigram count\n",
    "- Emission Probability\n",
    "- Transition Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_counts(sequences_A, sequences_B):\n",
    "    dcount = defaultdict(dict) \n",
    "    d = dict(Counter(list(zip(sequences_A, sequences_B))))\n",
    "    for key,value in d.items():\n",
    "        dcount[key[0]][key[1]] = value\n",
    "    return dcount\n",
    "\n",
    "tags = [tag for word, tag in data.stream()]\n",
    "words = [word for word, tag in data.stream()]\n",
    "## Emission_count is a dictionary with Key as POS tags and value as a dictionary with key as word and value as count\n",
    "## Example {\"VERB\" : {'had': 5101,'thanked': 6} ...}\n",
    "## Emission count is used for calculating Emission Probabilities example: what is probability of \"had\" being a \"VERB\"\n",
    "emission_counts = pair_counts(tags, words)\n",
    "\n",
    "## Unigram Count\n",
    "def unigram_counts(sequences):\n",
    "    return(dict(Counter(sequences)))\n",
    "\n",
    "tag_unigrams = unigram_counts(tags)\n",
    "\n",
    "## Bigram Count \n",
    "import itertools\n",
    "def pairwise(iterable):\n",
    "    t, t_1 = itertools.tee(iterable)\n",
    "    next(t_1, 'end')\n",
    "    return zip(t, t_1)\n",
    "def bigram_counts(sequences):\n",
    "    return(dict(Counter(pairwise(sequences))))\n",
    "\n",
    "## Used to calculate transition probabilities \n",
    "## gives count of sequences like (NOUN,NOUN) , (NOUN,VERB) etc\n",
    "tag_bigrams = bigram_counts(tags)\n",
    "\n",
    "## Count of each sentence starting with a POS (Calculating Transtition Probability for (<start>,POS))\n",
    "def starting_counts(sequences):\n",
    "    return(dict(Counter([i[0] for i in sequences])))\n",
    "tags_label= data.Y\n",
    "tag_starts = starting_counts(tags_label)\n",
    "\n",
    "## Count of each sentence ending with a POS (Calculating Transtition Probability for (POS,<END>))\n",
    "def ending_counts(sequences):\n",
    "    return(dict(Counter([i[-1] for i in sequences])))\n",
    "\n",
    "tag_ends = ending_counts(tags_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Word Count for each POS\n",
    "print(\"There are {} words as {} \".format(tag_unigrams['NOUN'],'NOUN'))\n",
    "print(\"There are {} words as {} \".format(tag_unigrams['ADJ'],'ADJ'))\n",
    "print(\"There are {} words as {} \".format(tag_unigrams['ADV'],'ADV'))\n",
    "print(\"There are {} words as {} \".format(tag_unigrams['ADP'],'ADP'))\n",
    "print(\"There are {} words as {} \".format(tag_unigrams['CONJ'],'CONJ'))\n",
    "print(\"There are {} words as {} \".format(tag_unigrams['DET'],'DET'))\n",
    "print(\"There are {} words as {} \".format(tag_unigrams['NUM'],'NUM'))\n",
    "print(\"There are {} words as {} \".format(tag_unigrams['PRON'],'PRON'))\n",
    "print(\"There are {} words as {} \".format(tag_unigrams['PRT'],'PRT'))\n",
    "print(\"There are {} words as {} \".format(tag_unigrams['VERB'],'VERB'))\n",
    "print(\"There are {} words as {} \".format(tag_unigrams['X'],'X'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HMM Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model = HiddenMarkovModel(name=\"base-hmm-tagger\")\n",
    "tags = [tag for _, tag in data.stream()]\n",
    "words = [word for word, _ in data.stream()]\n",
    "emission_counts = pair_counts(tags, words)\n",
    "states = {}\n",
    "\n",
    "for pos_tag in data.tagset:\n",
    "    emission_probabilities = dict()\n",
    "    \n",
    "    for word, occurance in emission_counts[pos_tag].items(): \n",
    "        emission_probabilities[word] = occurance / tag_unigrams[pos_tag] \n",
    "\n",
    "    tag_distribution = DiscreteDistribution(emission_probabilities) \n",
    "    state = State(tag_distribution, name=pos_tag)\n",
    "    states[pos_tag] = state\n",
    "    basic_model.add_state(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pos_tag in data.tagset:\n",
    "    state = states[pos_tag]\n",
    "    tags_label= data.Y\n",
    "    tag_starts = starting_counts(tags_label)\n",
    "    tag_ends = ending_counts(tags_label)\n",
    "    start_probability = tag_starts[pos_tag] / sum(tag_starts.values())\n",
    "    basic_model.add_transition(basic_model.start, state, start_probability)\n",
    "    end_probability = tag_ends[pos_tag] / sum(tag_ends.values())\n",
    "    basic_model.add_transition(state, basic_model.end, end_probability)\n",
    "\n",
    "for tag_1 in data.tagset:\n",
    "    state_1 = states[tag_1]\n",
    "    sum_of_probabilities = 0\n",
    "    for tag_2 in data.tagset:\n",
    "        state_2 = states[tag_2]\n",
    "        bigram = (tag_1, tag_2)\n",
    "        transition_probability = tag_bigrams[bigram] / tag_unigrams[tag_1]\n",
    "        sum_of_probabilities += transition_probability\n",
    "        basic_model.add_transition(state_1, state_2, transition_probability)    \n",
    "    \n",
    "basic_model.bake()\n",
    "print(\"Number of nodes or states: \", basic_model.node_count())\n",
    "print(\"Number of edges: \", basic_model.edge_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Accuracy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unknown(sequence):\n",
    "    return [w if w in data.training_set.vocab else 'nan' for w in sequence]\n",
    "def simplify_decoding(X, model):\n",
    "    _, state_path = model.viterbi(replace_unknown(X))\n",
    "    return [state[1].name for state in state_path[1:-1]]  # do not show the start/end state predictions\n",
    "def accuracy(X, Y, model):\n",
    "    correct = total_predictions = 0\n",
    "    for observations, actual_tags in zip(X, Y):\n",
    "        try:\n",
    "            most_likely_tags = simplify_decoding(observations, model)\n",
    "            correct += sum(p == t for p, t in zip(most_likely_tags, actual_tags))\n",
    "        except:\n",
    "            pass\n",
    "        total_predictions += len(observations)\n",
    "    return correct / total_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hmm_training_acc = accuracy(data.training_set.X, data.training_set.Y, basic_model)\n",
    "print(\"training accuracy basic hmm model: {:.2f}%\".format(100 * hmm_training_acc))\n",
    "hmm_testing_acc = accuracy(data.testing_set.X, data.testing_set.Y, basic_model)\n",
    "print(\"testing accuracy basic hmm model: {:.2f}%\".format(100 * hmm_testing_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
